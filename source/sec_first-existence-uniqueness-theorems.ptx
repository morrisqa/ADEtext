<?xml version="1.0" encoding="UTF-8"?>

<section xml:id="sec-existencethms">
  <title>Existence &amp; Uniquess Theorem</title>
  <subsection xml:id="subsec-prelim">
    <title>Some Preliminaries</title>
    <p>
      Before we can discuss existence and uniqueness theorems for initial value problems, we must first spend some time discussing properties of functions and sets of functions.  Before we can proceed, however, it is important that we are clear about what a differential equation is and what we mean when we say that a function is a solution to a particular differential equation.
      <definition xml:id="def-de">
       <statement>
         <p>
           An ordinary differential equation of order <m>n</m> is an equation of the form
           <me>
             F(t,x(t),x'(t), \ldots, x^{(n)}(t))=0,
           </me>
           where <m>F:\mathbb{R}^{n+1} \to \mathbb{R}</m> is a function.  The differential equation is said to be in normal form if it can be written in the form
           <me>
             x^{(n)}(t)=f(t,x(t),x'(t), \ldots, x^{(n-1)}(t)).
           </me>
         </p>
       </statement>
     </definition>
    <p>
      Every differential equation of order <m>n</m> which is written in normal form can be converted into an <m>n</m>-dimensional system of first order differential equations, as we illustrate in the next example.
    </p>

    <example xml:id="ex1-1">
      <statement>
        <p>
          Rewrite the second-order differential equation
          <me>
            x''(t)  + x'(t) \cdot x(t) = 0
          </me>
          as a system of two first-order differential equations.

        </p>
      </statement>
      <solution>
        <p>
          Let <m>x_1(t)=x(t)</m> and <m>x_2(t)=x'(t)</m>.  Then <m>x_1'(t)=x'(t)=x_2(t)</m> and <m>x_2'(t)=x''(t)=-x'(t) \cdot x(t) = x_2(t)\cdot x_1(t)</m>.  Hence, we can rewrite the second-order differential equation as <me>
            \left\{ \begin{array}{l} x_1'(t)=x_2(t) \\ x_2'(t)=x_2(t)\cdot x_1(t) \end{array} \right.
          </me>

        </p>
      </solution>
    </example>

      In this section (and the next few sections), we will concern ourselves with initial value problems of the form

        <men xml:id="eq-picard">
          \left\{ \begin{array}{l} x^\prime = f(t,x) \\ x(\alpha)=\beta \end{array} \right.
        </men>
        where <m>\alpha,\beta \in \mathbb{R}</m> and <m>f:D \to \mathbb{R}^n</m> is a continuous function defined on an open set <m> D=I \times \mathbb{R} \subset \mathbb{R}^2</m> with <m>\alpha \in I</m>.
    </p>
    <p>
      It is reasonable to ask why we restrict ourselves to single first order ordinary differential equations.  There are two reasons: first, as we observed earlier, every differential equation of order <m>n</m> can be rewritten as a system of <m>n</m> first-order equations.  Working with systems, while not that different from single equations, does require a bit more knowledge of linear algebra.  So, instead of working with systems, we will work with single equations.  For those interested in systems, the good news is that not much changes in many results (or their associated proofs) from the single equation case.
    </p>

    </subsection>

    <subsection xml:id="subsec-">
      <title>Uniqueness of Solutions</title>
    <p>
      As <xref ref="ex2" /> showed, continuity of <m>\vec{f}</m> is not enough to guarantee the uniqueness of the solution. It's worth recalling the problem here, which was,
      <me>
        \left\{ \begin{array}{l} y'=y^{\frac{2}{3}} \\ y(0)=0 \end{array} \right.
      </me>
      Clearly we need to seek out a more stringent criteria for <m>f</m> than just continuity if we want to ensure that the initial value problem has a unique solution.  Our natural intuition from calculus may suggest to us that we try to establish a uniqueness theorem when <m>f</m> is differentiable.  This is quite a reasonable hypothesis, particularly since we note that the failure of uniqueness occurs when the initial value <m>y(0)=0</m> occurs where the function <m>f(y)=y^\frac{2}{3}</m> fails to be differentiable.
    </p>
    <p>
      But it is important to note here the way in which <m>f(y)</m> fails to be differentiable. In particular, <m>f(y)</m> is differentiable for all <m>y\neq 0</m> and <m>\displaystyle \lim_{y \to 0^+}f'(y)\lim_{y \to 0^+}f'(y)=\infty</m>.  What is unknown to us now, then, is whether it is the simple failure of the derivative exist that is the issue, or whether it is the unboundedness of the derivative that is the issue. We will turn our attention shortly to an example in order to help clarify matters for us.  But first, we begin with an important and useful inequality.
    </p>
    <theorem xml:id="thm-gronwall">
      <title>Gronwall's Inequality</title>
      <statement>
        <p>
          Let <m>f:[a,b] \to \mathbb{R} </m> be a non-negative function, and let there exist constants <m>A,B \ge 0</m> such that <m>f</m> satisfies
          <me>
            f(t) \le A + B \int_a^t f(s) \; ds
          </me>
          for all <m>t \in [a,b]</m>.  Then <m>f(t) \le A e^{B(t-a)}</m> for all <m>t \in [a,b]</m>.

        </p>
      </statement>
      <proof>
        <p>
          Define <m>Q:[a,b] \to \mathbb{R}</m> by <m>Q(t):=A + B \int_a^t f(s) \; ds</m> so that <m>f(t)\le Q(t)</m> for all <m>t \in [a,b]</m>.  then
          <me>
          Q'(t)=Bf(t) \le B Q(t) \; \; \; \forall t \in [a,b],
          </me>
          or equivalently, <m>Q'(t)-BQ(t) \le 0</m> for all <m>t \in [a,b]</m>.  Mulitplying both sides of the inequality by <m>e^{-B(t-a)}</m> yields
          <me>
          Q'(t)e^{-B(t-a)} - B e^{-B(t-a)} Q(t) \le 0 \; \; \; \forall t \in [a,b],
          </me>
          or equivalently
          <me>
            (Q(t)e^{-B(t-a)})' \le 0 \; \; \; \forall t \in [a,b].
          </me>
          Stated another way, we have now established that the function <m>Q(t)e^{-B(t-a)}</m> is non-increasing on <m>[a,b]</m>, and hence
          <me>
            Q(t)e^{-B(t-a)} \le Q(a)e^{-B(a-a)}= A \; \; \; \forall t \in [a,b].
          </me>
          Hence, <m>Q(t) \le A e^{B(t-a)}</m>, and since <m>f(t) \le Q(t)</m> by assumption, we have the desired result, namely <m>f(t) \le A e^{B(t-a)}</m>.

        </p>
      </proof>
    </theorem>
    <p>
    At first glance, the theorem may seem of little use.  It only applies to non-negative functions which satisfy a very specific type of inequality.  However, as the next example will show us, this inequality is quite useful!
    </p>
    <example xml:id="exabsf">
      <statement>
        <p>
          Show that the initial value problem
          <me>
            \left\{ \begin{array}{c}
            y'(t)=|y(t)|, \\
            y(0)=0,
            \end{array} \right.
          </me>
        has exactly one solution.
        </p>
      </statement>
      <solution>
        <p>
          It is clear that <m>z(t)=0</m> is a solution for all <m>t \in \mathbb{R}</m>.  Assume there is another solution, say <m>w(t)</m>.  Using the Fundamental Theorem of Calculus, <m>z(t)</m> and <m>w(t)</m> may be written as
          <me>
            z(t)=z(0)-\int_0^t |z(s)| \; ds = \int_0^t |z(s)| \; ds
          </me>
          and
          <me>
            w(t)=w(0)-\int_0^t |w(s)| \; ds = \int_0^t |w(s)| \; ds,
          </me>
          respectively, for all <m>t \in \mathbb{R}</m>.  Then,
          <me>
            |w(t)-z(t)|= \left| \int_0^t |w(s)| \; ds - \int_0^t |z(s)| \; ds \right| = \left| \int_0^t |w(s)| - |z(s)| \; ds \right| \; \; \; \forall t \in \mathbb{R}.
          </me>
          and, utilizing the fact that <m>\left|\int_a^b g(s)\; ds \right| \le \int_a^b |g(s)| \; ds</m>, we have
          <me>
            |w(t)-z(t)| = \left| \int_0^t |w(s)| - |z(s)| \; ds \right| \le  \int_0^t \left| |w(s)| - |z(s)| \right| \; ds.
          </me>
          Finally, we apply the reverse triangle inequality to observe that
          <me>
            |w(t)-z(t)| \le  \int_0^t \left| |w(s)| - |z(s)| \right| \; ds \le \int_0^t \left| w(s) - z(s) \right| \; ds.
          </me>
          Now, since this inequality holds for all <m>t\in\mathbb{R}</m>, we may apply Gronwall's Inequality with <m>f(t)=|w(t)-z(t)|</m>, <m>A=0</m>, and <m>B=1</m>.  Hence, <m>|w(t)-z(t)|\le 0\cdot e^{1(t-0)}=0</m>, and since <m>|w(t)-z(t)|\ge 0</m> by definition of the absolute value function, we must have <m>|w(t)-z(t)|=0</m> for all <m>t \in \mathbb{R}</m>.  This implies that <m>w(t)=z(t)</m> for all <m>t \in \mathbb{R}</m>, and hence we only have one solution.
        </p>
      </solution>
    </example>
    <p>
      This is great news!  We have shown in <xref ref="exabsf" /> that (1) a solution exists (by finding a solution directly) and (2) that the solution we found is the only solution!  Additionally, the work provides a roadmap for proceeding toward a more general proof of a uniqueness theorem.  Before moving on, consider how the work in <xref ref="exabsf" /> would change if the right-hand side of the differential equation were changed from <m>|y(t)|</m> to a more general <m>f(t,y)</m>. While we would not know whether a solution existed for the more general equation, we could still endeavor to show that if we have two solutions, say <m>z(t)</m> and <m>w(t)</m>, then we must have <m>z(t)=w(t)</m>. Virtually all of our work would proceed the same, until we reached our use of the reverse triangle inequality.  In <xref ref="exabsf" />, we used
      <me>
        \left| | w(t)| - |z(t)| \right| \le \left| w(t)-z(t) \right|,
      </me>
      the reverse triangle inequality.  Now, however, we needed
      <me>
        \left| f(t,w(t))-f(t,z(t)) \right| \le K \left|w(t)-z(t)\right|.
      </me>
      Note that the inclusion of an additional constant <m>K>0</m> will not affect our work, since it will simply be included in the exponent when Gronwall's Inequality is applied.
    </p>
    <p>
      So, what functions, if any, satisfy this inequality and how we think of them?  Before we answer this question, let's give functions satisfying this condition a name.
    </p>
    <definition xml:id="def-lipschitz">
     <statement>
       <p>
         A function <m>f:I \times \mathbb{R} \to\mathbb{R}^n</m> is said to satisfy a <term>Lipschitz condition with respect to <m>x</m> </term> on the open set <m>I \times R \subset \mathbb{R}^2</m> if, for any rectangle
         <me>
           \Omega:=\{(t,x) \mid |t-t_0|\le a, \; \|x-x_0\| \le b \} \subset I \times \mathbb{R},
         </me>
         there exists a constant <m>K</m> (depending on <m>\Omega</m>) such that
         <me>
           \left|f(t,x)-f(t,y)\right|\le K|x-y|
         </me>
         for all <m>(t,x),(t,y) \in \Omega</m>.
       </p>
     </statement>
   </definition>
   <p>
     Having defined such things, it would be helpful now to convince ourselves that such functions actually exist!  What a terrible situation we would be in if the very type of functions we needed did not, in fact, exist!
   </p>
   <example xml:id="abs_is_lip">
     <statement>
       <p>
         Show that the function <m>f(t,y)=|y|</m> satisfies a Lipschitz condition with respect to <m>y</m> with <m>K=1</m>.
       </p>
     </statement>
     <solution>
       <p>
         By the reverse triangle inequality, we have
         <me>
           \left|f(t,x)-f(t,y)\right| = \left| |x|-|y| \right| \le |x-y|.
         </me>
         for all <m>(t,y)\in \mathbb{R}^2.</m>  Hence, <m>f(t,y)</m> satisfies a Lipschitz condition with <m>K=1.</m>
       </p>
     </solution>
   </example>
   <example>
     <statement>
       <p>
         Show that the function <m>f(t,y)=t^2 y^3</m> satisfies a Lipschitz condition.
       </p>
     </statement>
     <solution>
       <p>
         This example is a bit trickier than <xref ref="abs_is_lip" /> because our constant <m>K</m> will depend on <m>\Omega</m> in this case.  Let
         <me>
           \Omega:=\{(t,x) \mid |t-t_0|\le a, \; \|x-x_0\| \le b \} \subset I \times \mathbb{R}.
         </me>
         Then, for all <m>(t,y) \in \Omega</m>
         <me>
           \left|f(t,x)-f(t,y)\right| = \left| t^2x^3-t^2y^3 \right| =|t^2| |x^3-y^3|=|t^2||3\xi ^2| |x-y|.
         </me>
         for some <m>\xi \in (x_0-b,x_0+b)</m> by the Mean Value Theorem.  Let <m>A=max\{|t_0-a|,|t_0+a|\}</m> and let <m>B=max\{|x_0-b|,|x_0+b|\}</m>.  Then,
         <me>
           \left|f(t,x)-f(t,y)\right| = |t^2||3\xi ^2| |x-y| \le A^2\cdot 3B^2 |x-y|,
         </me>
         for all <m>(t,y)\in\Omega</m>.  Hence, <m>f(t,y)</m> satisfies a Lipschitz condition.
       </p>
     </solution>
   </example>
   <fact xml:id="fact-">
     <statement>
       <p>
         If <m>f(t,y)</m> has a continuous derivative with respect to <m>y</m>, then <m>f</m> satisfies a Lipschitz condition with respect to <m>y</m>.  A version of this statement will be proven in the exercises.
       </p>
     </statement>
   </fact>
    <p>
    It turns out that the issue is, in fact, the latter.  We can prove that an initial value problem has a unique solution if we assume continuity of <m>\vec{f}(t,\vec{x})</m> in both variables and boundedness of the partial derivative <m>\vec{f}_x(t,\vec{x})</m>.  But we can do better!
    </p>
    <p>
      As it turns out, the key necessary for existence and uniqueness is for <m>f(t,y)</m> to not change too rapidly as <m>y</m> varies.  There are many ways to quantify this (one being the boundedness of the partial derivative as discussed above), but we will use the notion of a Lipschitz condition, which we defined below.
    </p>


  </subsection>


  <theorem xml:id="thm-picard">
    <title>Picard's Existence &amp; Uniqueness Theorem</title>
    <statement>
        <p>
          If <m>\vec{f}</m> satisfies a Lipschitz condition  with respect to <m>x</m>, then <xref ref="eq-picard" /> has a unique solution in a neighborhood of <m>t=\alpha</m>.
        </p>
    </statement>
    <proof>
      <p>
        The proof of this theorem relies on the Contraction Mapping Theorem.
      </p>
    </proof>
  </theorem>
</section>
