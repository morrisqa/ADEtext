<?xml version="1.0" encoding="UTF-8"?>

<section xml:id="sec-existencethms">
  <title>Existence &amp; Uniquess Theorem</title>
  <subsection xml:id="subsec-prelim">
    <title>Some Preliminaries</title>
    <p>
      Before we can discuss existence and uniqueness theorems for initial value problems, we must first spend some time discussing properties of functions and sets of functions.  Before we can proceed, however, it is important that we are clear about what a differential equation is and what we mean when we say that a function is a solution to a particular differential equation.
      <definition xml:id="def-de">
       <statement>
         <p>
           An ordinary differential equation of order <m>n</m> is an equation of the form
           <me>
             F(t,x(t),x'(t), \ldots, x^{(n)}(t))=0,
           </me>
           where <m>F:\mathbb{R}^{n+1} \to \mathbb{R}</m> is a function.  The differential equation is said to be in normal form if it can be written in the form
           <me>
             x^{(n)}(t)=f(t,x(t),x'(t), \ldots, x^{(n-1)}(t)).
           </me>
         </p>
       </statement>
     </definition>
    <p>
      Every differential equation of order <m>n</m> which is written in normal form can be converted into an <m>n</m>-dimensional system of first order differential equations, as we illustrate in the next example.
    </p>

    <example xml:id="ex1-1">
      <statement>
        <p>
          Rewrite the second-order differential equation
          <me>
            x''(t)  + x'(t) \cdot x(t) = 0
          </me>
          as a system of two first-order differential equations.

        </p>
      </statement>
      <solution>
        <p>
          Let <m>x_1(t)=x(t)</m> and <m>x_2(t)=x'(t)</m>.  Then <m>x_1'(t)=x'(t)=x_2(t)</m> and <m>x_2'(t)=x''(t)=-x'(t) \cdot x(t) = x_2(t)\cdot x_1(t)</m>.  Hence, we can rewrite the second-order differential equation as <me>
            \left\{ \begin{array}{l} x_1'(t)=x_2(t) \\ x_2'(t)=x_2(t)\cdot x_1(t) \end{array} \right.
          </me>

        </p>
      </solution>
    </example>

      In this section (and the next few sections), we will concern ourselves with initial value problems of the form

        <men xml:id="eq-picard">
          \left\{ \begin{array}{l} x^\prime = \vec{f}(t,x) \\ x(\alpha)=\beta \end{array} \right.
        </men>
        where <m>\alpha,\beta \in \mathbb{R}</m> and <m>f:D \to \mathbb{R}^n</m> is a continuous function defined on an open set <m> D=I \times \mathbb{R} \subset \mathbb{R}^2</m> containing <m>(\alpha,\beta)</m>.
    </p>
    <p>
      It is reasonable to ask why we restrict ourselves to single first order ordinary differential equations.  There are two reasons: first, as we observed earlier, every differential equation of order <m>n</m> can be rewritten as a system of <m>n</m> first-order equations.  Working with systems, while not that different from single equations, does require a bit more knowledge of linear algebra.  So, instead of working with systems, we will work with single equations.  For those interested in systems, the good news is that not much changes in many results (or their associated proofs) from the single equation case.
    </p>
    <p>
      As <xref ref="ex2" /> showed, continuity of <m>\vec{f}</m> is not enough to guarantee the uniqueness of the solution. It's worth recalling the problem here, which was,
      <me>
        \left\{ \begin{array}{l} y'=y^{\frac{2}{3}} \\ y(0)=0 \end{array} \right.
      </me>
      Clearly we need to seek out a more stringent criteria for <m>f</m> than just continuity if we want to ensure that the initial value problem has a unique solution.  Our natural intuition from calculus may suggest to us that we try to establish a uniqueness theorem when <m>f</m> is differentiable.  This is quite a reasonable hypothesis, particularly since we note that the failure of uniqueness occurs when the initial value <m>y(0)=0</m> occurs where the function <m>f(y)=y^\frac{2}{3}</m> fails to be differentiable.
    </p>
    <p>
      But it is important to note here the way in which <m>f(y)</m> fails to be differentiable. In particular, <m>f(y)</m> is differentiable for all <m>y\neq 0</m> and <m>\displaystyle \lim_{y \to 0^+}f'(y)\lim_{y \to 0^+}f'(y)=\infty</m>.  What is unknown to us now, then, is whether it is the simple failure of the derivative exist that is the issue, or whether it is the unboundedness of the derivative that is the issue. We will turn our attention shortly to an example in order to help clarify matters for us.  But first, we begin with an important and useful inequality.
    </p>
    <theorem xml:id="thm-gronwall">
      <title>Gronwall's Inequality</title>
      <statement>
        <p>
          Let <m>f:[a,b] \to \mathbb{R} </m> be a non-negative function, and let there exist constants <m>A,B \ge 0</m> such that <m>f</m> satisfies
          <me>
            f(t) \le A + B \int_a^t f(s) \; ds
          </me>
          for all <m>t \in [a,b]</m>.  Then <m>f(t) \le A e^{B(t-a)}</m> for all <m>t \in [a,b]</m>.

        </p>
      </statement>
      <proof>
        <p>
          Define <m>Q:[a,b] \to \mathbb{R}</m> by <m>Q(t):=A + B \int_a^t f(s) \; ds</m> so that <m>f(t)\le Q(t)</m> for all <m>t \in [a,b]</m>.  then
          <me>
          Q'(t)=Bf(t) \le B Q(t) \; \; \; \forall t \in [a,b],
          </me>
          or equivalently, <m>Q'(t)-BQ(t) \le 0</m> for all <m>t \in [a,b]</m>.  Mulitplying both sides of the inequality by <m>e^{-B(t-a)}</m> yields
          <me>
          Q'(t)e^{-B(t-a)} - B e^{-B(t-a)} Q(t) \le 0 \; \; \; \forall t \in [a,b],
          </me>
          or equivalently
          <me>
            (Q(t)e^{-B(t-a)})' \le 0 \; \; \; \forall t \in [a,b].
          </me>
          Stated another way, we have now established that the function <m>Q(t)e^{-B(t-a)}</m> is non-increasing on <m>[a,b]</m>, and hence
          <me>
            Q(t)e^{-B(t-a)} \le Q(a)e^{-B(a-a)}= A \; \; \; \forall t \in [a,b].
          </me>
          Hence, <m>Q(t) \le A e^{B(t-a)}</m>, and since <m>f(t) \le Q(t)</m> by assumption, we have the desired result, namely <m>f(t) \le A e^{B(t-a)}</m>.

        </p>
      </proof>
    </theorem>
    <p>
    At first glance, the theorem may seem of little use.  It only applies to non-negative functions which satisfy a very specific type of inequality.  However, as the next example will show us, this inequality is quite useful!
    </p>
    <example xml:id="exabsf">
      <statement>
        <p>
          Show that the initial value problem
          <me>
            \left\{ \begin{array}{c}
            y'(t)=|y(t)|, \\
            y(0)=0,
            \end{array} \right.
          </me>
        has exactly one solution.
        </p>
      </statement>
      <solution>
        <p>
          It is clear that <m>z(t)=0</m> is a solution for all <m>t \in \mathbb{R}</m>.  Assume there is another solution, say <m>w(t)</m>.  Using the Fundamental Theorem of Calculus, <m>z(t)</m> and <m>w(t)</m> may be written as
          <me>
            z(t)=z(0)-\int_0^t |z(s)| \; ds = \int_0^t |z(s)| \; ds
          </me>
          and
          <me>
            w(t)=w(0)-\int_0^t |w(s)| \; ds = \int_0^t |w(s)| \; ds,
          </me>
          respectively, for all <m>t \in \mathbb{R}</m>.  Then,
          <me>
            |w(t)-z(t)|= \left| \int_0^t |w(s)| \; ds - \int_0^t |z(s)| \; ds \right| = \left| \int_0^t |w(s)| - |z(s)| \; ds \right| \; \; \; \forall t \in \mathbb{R}.
          </me>
          and, utilizing the fact that <m>\left|\int_a^b g(s)\; ds \right| \le \int_a^b |g(s)| \; ds</m>, we have
          <me>
            |w(t)-z(t)| = \left| \int_0^t |w(s)| - |z(s)| \; ds \right| \le  \int_0^t \left| |w(s)| - |z(s)| \right| \; ds.
          </me>
          Finally, we apply the reverse triangle inequality to observe that
          <me>
            |w(t)-z(t)| \le  \int_0^t \left| |w(s)| - |z(s)| \right| \; ds \le \int_0^t \left| w(s) - z(s) \right| \; ds.
          </me>
          Now, since this inequality holds for all <m>t\in\mathbb{R}</m>, we may apply Gronwall's Inequality with <m>f(t)=|w(t)-z(t)|</m>, <m>A=0</m>, and <m>B=1</m>.  Hence, <m>|w(t)-z(t)|\le 0\cdot e^{1(t-0)}=0</m>, and since <m>|w(t)-z(t)|\ge 0</m> by definition of the absolute value function, we must have <m>|w(t)-z(t)|=0</m> for all <m>t \in \mathbb{R}</m>.  This implies that <m>w(t)=z(t)</m> for all <m>t \in \mathbb{R}</m>, and hence we only have one solution.
        </p>
      </solution>
    </example>
    <p>
    It turns out that the issue is, in fact, the latter.  We can prove that an initial value problem has a unique solution if we assume continuity of <m>\vec{f}(t,\vec{x})</m> in both variables and boundedness of the partial derivative <m>\vec{f}_x(t,\vec{x})</m>.  But we can do better!
    </p>
    <p>
      As it turns out, the key necessary for existence and uniqueness is for <m>f(t,y)</m> to not change too rapidly as <m>y</m> varies.  There are many ways to quantify this (one being the boundedness of the partial derivative as discussed above), but we will use the notion of a Lipschitz condition, which we defined below.
    </p>
     <definition xml:id="def-lipschitz">
      <statement>
        <p>
          A function <m>\vec{f}:D\to\mathbb{R}^n</m> is said to satisfy a <em>Lipschitz condition with respect to <m>x</m> </em> on the open set <m>D \subset \mathbb{R} \times \mathbb{R}^n</m> if, for any rectangle
          <me>
            \Omega:=\{(t,\vec{x}) \mid |t-t_0|\le a, \; \|\vec{x}-\vec{x}_0\| \le b \} \subset D,
          </me>
          there exists a constant <m>K</m> (depending on <m>\Omega</m>) such that
          <me>
            \|\vec{f}(t,\vec{x})-\vec{f}(t,\vec{y})\|\le K\|\vec{x}-\vec{y}\|
          </me>
          for all <m>(t,\vec{x}),(t,\vec{y}) \in \Omega</m>.
        </p>
      </statement>
    </definition>

  </subsection>


  <theorem xml:id="thm-picard">
    <title>Picard's Existence &amp; Uniqueness Theorem</title>
    <statement>
        <p>
          If <m>\vec{f}</m> satisfies a Lipschitz condition  with respect to <m>x</m>, then <xref ref="eq-picard" /> has a unique solution in a neighborhood of <m>t=\alpha</m>.
        </p>
    </statement>
    <proof>
      <p>
        The proof of this theorem relies on the Contraction Mapping Theorem.
      </p>
    </proof>
  </theorem>
</section>
